# ðŸ” ANALYSE : Utilisation des donnÃ©es faciales dans NOKTA ONE

## ðŸ“Š Ã‰tat actuel vs IdÃ©al

### âœ… Ce qui est fait actuellement

#### 1. **Capture des donnÃ©es faciales (MediaPipe)**
- âœ… 468 landmarks du visage dÃ©tectÃ©s
- âœ… Bounding box et position dans l'ovale
- âœ… StabilitÃ© et mouvement
- âœ… Confidence score
- âŒ **Blendshapes dÃ©sactivÃ©s** (`outputFaceBlendshapes: false`)

#### 2. **Analyse de l'image (GPT-4 Vision)**
- âœ… Analyse visuelle de l'image complÃ¨te
- âœ… Extraction de signaux physiologiques :
  - `eye_openness` (0.0-1.0)
  - `blink_rate` (0.0-1.0)
  - `jaw_tension` (0.0-1.0)
  - `lip_compression` (0.0-1.0)
  - `forehead_tension` (0.0-1.0)
  - `head_stability` (0.0-1.0)
- âœ… Classification en 3 Ã©tats internes : `HIGH_ACTIVATION`, `LOW_ENERGY`, `REGULATED`

#### 3. **SÃ©lection de micro-action**
- âœ… Mapping Ã©tat â†’ actions (`STATE_TO_ACTIONS`)
- âœ… Anti-rÃ©pÃ©tition (Ã©vite la derniÃ¨re action)
- âœ… PondÃ©ration par feedback historique
- âŒ **N'utilise PAS les donnÃ©es faciales brutes** (landmarks, blendshapes)
- âŒ **N'utilise PAS les donnÃ©es contextuelles** (HRV, sommeil, mÃ©tÃ©o)

---

## âŒ Ce qui MANQUE pour une vraie intelligence

### ProblÃ¨me 1 : Les landmarks ne sont PAS utilisÃ©s
**Fichier** : `lib/hooks/useFaceDetection.ts`
- Les 468 landmarks sont capturÃ©s mais **uniquement pour l'affichage visuel**
- Ils ne sont **jamais envoyÃ©s** Ã  l'API d'analyse
- Ils ne sont **jamais stockÃ©s** dans Supabase

### ProblÃ¨me 2 : Les blendshapes sont dÃ©sactivÃ©s
**Fichier** : `lib/hooks/useFaceDetection.ts` (ligne 137)
```typescript
outputFaceBlendshapes: false, // â† DÃ‰SACTIVÃ‰
```
**Impact** : Impossible de dÃ©tecter les Ã©motions rÃ©elles (stress, fatigue, joie, calme)

### ProblÃ¨me 3 : Pas de donnÃ©es contextuelles
**Manque** :
- HRV (Heart Rate Variability)
- Heures de sommeil
- QualitÃ© du sommeil
- Pas quotidiens
- MÃ©tÃ©o
- Fuseau horaire

### ProblÃ¨me 4 : Algorithme trop simple
**Fichier** : `lib/skane/selector.ts`
- SÃ©lection basÃ©e **uniquement** sur :
  1. L'Ã©tat interne (HIGH/LOW/REGULATED)
  2. Le feedback historique
  3. Anti-rÃ©pÃ©tition
- **Aucun scoring multi-factoriel** (Ã©motions + biomÃ©trie + contexte)

---

## ðŸŽ¯ PLAN D'ACTION : ImplÃ©mentation d'une vraie intelligence

### Ã‰tape 1 : Activer Face Blendshapes âš¡ PRIORITÃ‰ 1

**Fichier** : `lib/hooks/useFaceDetection.ts`

**Changement** :
```typescript
outputFaceBlendshapes: true, // â† ACTIVER
```

**RÃ©sultat** : 52 expressions faciales disponibles :
- `browDownLeft`, `browDownRight` â†’ Stress/Concentration
- `eyeSquintLeft`, `eyeSquintRight` â†’ Fatigue
- `mouthSmileLeft`, `mouthSmileRight` â†’ Joie
- `jawOpen` â†’ Surprise/DÃ©tente
- ... 47 autres

---

### Ã‰tape 2 : CrÃ©er un algorithme d'Ã©motion âš¡ PRIORITÃ‰ 1

**Nouveau fichier** : `lib/emotion-detection.ts`

**Fonction** : `calculateEmotions(blendshapes: FaceBlendshapes)`

**Calcul** :
```typescript
stress = (
  (browDownLeft + browDownRight) / 2 * 0.4 +
  (1 - jawOpen) * 0.3 +
  (1 - mouthSmileLeft - mouthSmileRight) / 2 * 0.3
);

fatigue = (
  (eyeSquintLeft + eyeSquintRight) / 2 * 0.5 +
  (eyeBlinkLeft + eyeBlinkRight) / 2 * 0.3 +
  (1 - (eyeWideLeft + eyeWideRight) / 2) * 0.2
);

joy = (
  (mouthSmileLeft + mouthSmileRight) / 2 * 0.6 +
  (cheekSquintLeft + cheekSquintRight) / 2 * 0.4
);

calm = (
  jawOpen * 0.3 +
  (1 - stress) * 0.4 +
  (1 - fatigue) * 0.3
);
```

---

### Ã‰tape 3 : Enrichir le schÃ©ma Supabase âš¡ PRIORITÃ‰ 2

**Fichier** : `supabase/schema.sql`

**Ajouts** :
```sql
ALTER TABLE skane_sessions ADD COLUMN face_blendshapes JSONB;
ALTER TABLE skane_sessions ADD COLUMN emotions JSONB; -- {stress: 0.78, fatigue: 0.45, joy: 0.12}
ALTER TABLE skane_sessions ADD COLUMN dominant_emotion TEXT;
ALTER TABLE skane_sessions ADD COLUMN time_of_day TEXT;
ALTER TABLE skane_sessions ADD COLUMN day_of_week INTEGER;
ALTER TABLE skane_sessions ADD COLUMN hrv_value FLOAT;
ALTER TABLE skane_sessions ADD COLUMN sleep_hours FLOAT;
ALTER TABLE skane_sessions ADD COLUMN sleep_quality INTEGER;
ALTER TABLE skane_sessions ADD COLUMN steps_today INTEGER;
ALTER TABLE skane_sessions ADD COLUMN weather JSONB;
ALTER TABLE skane_sessions ADD COLUMN location_timezone TEXT;
```

---

### Ã‰tape 4 : CrÃ©er un algorithme de recommandation intelligent âš¡ PRIORITÃ‰ 2

**Nouveau fichier** : `lib/micro-action-algorithm.ts`

**Fonction** : `recommendMicroAction(input: RecommendationInput)`

**Scoring multi-factoriel** :
1. **Ã‰motion dominante** (40%) â†’ Actions ciblÃ©es
2. **BiomÃ©trie** (30%) â†’ Ajustements HRV/sommeil
3. **Contexte** (20%) â†’ Moment de la journÃ©e
4. **Historique** (10%) â†’ PrÃ©fÃ©rences utilisateur

**Exemple** :
```typescript
if (emotions.stress > 0.6) {
  actionScores['breathing_4_7_8'] = 0.9;
  actionScores['body_scan'] = 0.7;
}
if (biometrics?.hrv < 40) {
  actionScores['breathing_4_7_8'] += 0.3; // HRV bas = stress
}
if (context.timeOfDay === 'morning') {
  actionScores['power_pose'] += 0.2;
}
```

---

### Ã‰tape 5 : IntÃ©grer dans l'API d'analyse âš¡ PRIORITÃ‰ 3

**Fichier** : `app/api/skane/analyze/route.ts`

**Changements** :
1. Accepter `faceBlendshapes` dans le body
2. Calculer les Ã©motions avec `calculateEmotions()`
3. Enrichir avec donnÃ©es contextuelles (HRV, sommeil, mÃ©tÃ©o)
4. Utiliser `recommendMicroAction()` au lieu de `selectMicroAction()`
5. Stocker toutes les donnÃ©es dans Supabase

---

## ðŸ“ˆ Impact attendu

### Avant (actuel)
- âŒ DÃ©tection basÃ©e uniquement sur GPT-4 Vision (image statique)
- âŒ Pas de donnÃ©es faciales brutes
- âŒ Pas d'Ã©motions rÃ©elles
- âŒ Pas de contexte biomÃ©trique
- âŒ Algorithme simple (Ã©tat â†’ action)

### AprÃ¨s (avec amÃ©liorations)
- âœ… DÃ©tection basÃ©e sur blendshapes (52 expressions)
- âœ… Ã‰motions calculÃ©es (stress, fatigue, joie, calme)
- âœ… Contexte biomÃ©trique (HRV, sommeil, activitÃ©)
- âœ… Algorithme multi-factoriel (Ã©motions + biomÃ©trie + contexte + historique)
- âœ… Recommandations personnalisÃ©es et prÃ©cises

---

## ðŸš€ Ordre d'implÃ©mentation recommandÃ©

1. **Activer blendshapes** (5 min) â†’ DonnÃ©es disponibles
2. **CrÃ©er algorithme d'Ã©motion** (30 min) â†’ Calcul des Ã©motions
3. **CrÃ©er algorithme de recommandation** (1h) â†’ Scoring intelligent
4. **Enrichir schÃ©ma Supabase** (15 min) â†’ Stockage des donnÃ©es
5. **IntÃ©grer dans l'API** (1h) â†’ Utilisation complÃ¨te

**Total estimÃ©** : ~3h de dÃ©veloppement

---

## ðŸ“ Notes importantes

- **MediaPipe Face Landmarker** ne dÃ©tecte PAS les Ã©motions directement
- Il faut utiliser **Face Blendshapes** (52 valeurs) pour calculer les Ã©motions
- Les blendshapes sont disponibles dans MediaPipe mais **dÃ©sactivÃ©s** actuellement
- GPT-4 Vision peut analyser l'image, mais les blendshapes sont **plus prÃ©cis** pour les micro-expressions

---

## ðŸ”— RÃ©fÃ©rences

- [MediaPipe Face Blendshapes](https://developers.google.com/mediapipe/solutions/vision/face_landmarker#blendshapes)
- [MediaPipe Face Landmarker Task](https://developers.google.com/mediapipe/solutions/vision/face_landmarker)
- [NOKTA ONE Flow Implementation](./FLOW_IMPLEMENTATION.md)
